---
icon: edit
date: 2023-10-25
category:
  - 项目总结
  - 学习笔记
tag:
  - 笔记
star: false
sticky: false
copy.disableCopy: true
---

# 文件分片上传和断点续传

## 文件切片上传的整体思路

前端大文件上传核心是`利用 Blob.prototype.slice` 方法，和数组的 slice 方法相似，文件的 slice 方法可以返回`原文件的某个切片`。

预先定义好单个切片大小，将文件切分为一个个切片，然后借助 http 的可并发性，同时上传多个切片。这样从原本传一个大文件，变成了`并发`传多个小的文件切片，可以大大减少上传时间。

另外由于是并发，传输到服务端的顺序可能会发生变化，因此我们还需要给每个切片记录顺序

<!-- more -->

## 切片上传的流程

- 对文件进行切片
- 将切片文件传输给服务端

```js
//创建分片，固定分片大小
function creatChunk(file, size = 2 * 1024 * 1024) {
    //收集分片
    const chunk = [];
    let cur = 0;
    while (cur <= files.size) {
        chunk.push({
            file: files.slice(cur, cur + size)//调用slice函数进行切片
        });
        cur += size;
    }
    return chunk;
}
// 监听上传点击事件
upload.addEventListener('click', e => {
    // 切片上传,将每一个分片打包为一个文件名 + index的数据包
    const uploadList = chunks.map(({ file }, index) => ({
        file,
        size: file.size,
        precent: 0,
        chunkName: `${files.name}-${index}`,
        fileName: files.name,
        index
    }))
    uploadFile(uploadList)
})
// 上传所有分片
function uploadFile(list) {
    // 处理分片
    const requestList = list.map(({ chunkName,file, fileName,index}) => {
        let formData = new FormData();
		// 将每一个分片打包成一个FormData对象传给后台
        formData.append('file', file);
        formData.append('fileName', fileName);
        formData.append('chunkName', chunkName);
        return { formData, index }
    })
    .map(({ formData }) =>
          this.request({
                url: "http://localhost:3000",
                data: formData
          })
    );
}
```

## 利用worker线程进行文件切片并通过调用库 `spark-md5`计算出文件的 hash 值

然而这里就存在一个问题，如果文件体积特别大的话，对文件进行分片的过程中会造成页面的卡顿。众所周知，js是单线程模型，如果这个计算过程在主线程中的话，那我们的页面必然会直接崩溃，这时，就该 **`Web Worker`** 来上场了。

Web Worker 的作用，就是为 JavaScript 创造多线程环境，允许主线程创建 Worker 线程，将一些任务分配给后者运行。在主线程运行的同时，Worker 线程在后台运行，两者互不干扰。具体的作用，不了解的同学可以自行去学些一下。这里就不展开讲了。

```js
// 主线程的内容
// 创建一个worker对象
const worker = new worker('worker.js')
// 向子线程发送消息，并传入文件对象和切片大小，开始计算分割切片
worker.postMessage(file, size)

// 子线程计算完成后，会将切片返回主线程
worker.onmessage = (chunks) => {
	...
}
```

Worker子线程：

```js
// 导入脚本
self.importScripts("/spark-md5.min.js");
// 生成文件 hash
// 接收文件对象及切片大小
onmessage (file, size) => {
    let blobSlice = File.prototype.slice || File.prototype.mozSlice || File.prototype.webkitSlice,
      chunks = Math.ceil(file.size / size),
      currentChunk = 0,
      spark = new SparkMD5.ArrayBuffer(),
      fileReader = new FileReader();
    fileReader.onload = function (e) {
          console.log('read chunk nr', currentChunk + 1, 'of');
          const chunk = e.target.result;
          spark.append(chunk);
          currentChunk++;

          if (currentChunk < chunks) {
              //只要当前分片的数量小于文件应有的分片数量，那么就继续执行loadNext()
              loadNext();
          } else {
              // 当递归完所有的文件切片后，调用spark.end()生成这个文件的hash值
              let fileHash = spark.end();
              console.info('finished computed hash', fileHash);
              // 此处为重点，计算完成后，仍然通过postMessage通知主线程，将hash值和文件切片传给主线程
              postMessage({ fileHash, fileReader })
              self.close();
          }
    };
    fileReader.onerror = function () {
      	console.warn('oops, something went wrong.');
    };
    function loadNext() {
          let start = currentChunk * size,
            end = ((start + size) >= file.size) ? file.size : start + size;
          let chunk = blobSlice.call(file, start, end);
          fileReader.readAsArrayBuffer(chunk);
    }
    loadNext();
}
```

::: tip

在 worker 线程中，接受文件切片 fileChunkList，利用 fileReader 读取每个切片的 ArrayBuffer 并不断传入 spark-md5 中，每计算完一个切片通过 postMessage 向主线程发送一个进度事件，全部完成后将最终的 hash 发送给主线程。

:::

## 断点续传

在拿到切片和md5后，我们首先去服务器查询一下，是否已经存在当前文件。

1. 如果已存在，并且已经是上传成功的文件，则直接返回前端上传成功，即可实现"秒传"。
2. 如果已存在，并且有一部分切片上传失败，则返回给前端已经上传成功的切片name，前端拿到后，根据返回的切片，计算出未上传成功的剩余切片，然后把剩余的切片继续上传，即可实现"断点续传"。
3. 如果不存在，则开始上传，这里需要注意的是，在并发上传切片时，需要控制并发量，避免一次性上传过多切片，导致崩溃。

```js
// 检查是否已存在相同文件
async function checkAndUploadChunk(chunkList, fileMd5Value) {
    const requestList = []
    // 如果不存在，则上传
    for (let i = 0; i < chunkList; i++) {
     	requestList.push(upload({ chunkList[i], fileMd5Value, i }))
    }

    // 并发上传
    if (requestList?.length) {
     	await Promise.all(requestList)
    }
}

// 上传chunk
function upload({ chunkList, chunk, fileMd5Value, i }) {
    current = 0
    let form = new FormData()
    form.append("data", chunk) //切片流
    form.append("total", chunkList.length) //总片数
    form.append("index", i) //当前是第几片     
    form.append("fileMd5Value", fileMd5Value)
    return axios({
        method: 'post',
        url: BaseUrl + "/upload",
        data: form
    }).then(({ data }) => {
        if (data.stat) {
          current = current + 1
          // 获取到上传的进度
          const uploadPercent = Math.ceil((current / chunkList.length) * 100)
        }
    })
}

```

## 总结

::: tip

我们要实现一个大文件切片上传和断点续传的工作，首先前端需要做到的是获取input标签拿到的文件，对该文件进行切片，但因为是大文件，如果我们在js主线程中执行切片工作，那么可能会导致页面卡顿甚至崩溃，于是我们将切片工作交给了web Worker，这是HTML5 中提出的概念，分为两种类型，专用线程（Dedicated Web Worker） 和共享线程（Shared Web Worker）。专用线程仅能被创建它的脚本所使用（一个专用线程对应一个主线程），而共享线程能够在不同的脚本中使用（一个共享线程对应多个主线程）。

在这里我们使用的是专用线程，我们通过new关键字创建一个worker线程，在线程再通过`worker.postMessage(file, size)`将文件内容，和固定的分片大小传给worker线程。

并且在worker线程的文件分片过程中，我们同时导入第三方库`spark-MD5`为该文件生成一个专属于该文件的一个hash值。

在worker线程中，我们首先计算出文件根据分片固定大小被分成多少给分片，还需要new一个`FileReader`对象，通过递归先使用`Blob`对象的`slice()`方法来对文件依次分片，最后我们使用`FileReader`的`readAsArrayBuffer`方法收集分片，递归的结束条件是递归次数即分片次数等于计算出应有的分片数量，我们首先调用`SparkMD5.ArrayBuffer()`的实例上的end方法，生成文件的hash值。接着我们就可以向主线程发送消息，消息内容包括{ fileHash, fileReader }即文件的hash值，和文件的分片。

在拿到切片和md5哈希值以后，我们首先去服务器查询一下，是否已经存在当前文件。

如果已存在，并且已经是上传成功的文件，则直接返回前端上传成功，即可实现"秒传"。

如果已存在，并且有一部分切片上传失败，则返回给前端已经上传成功的切片name，前端拿到后，根据返回的切片，计算出未上传成功的剩余切片，然后把剩余的切片继续上传，即可实现"断点续传"。

如果不存在，则开始上传，这里需要注意的是，在并发上传切片时，需要控制并发量，避免一次性上传过多切片，导致崩溃。

:::